{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#source  https://stackoverflow.com/questions/49277926/python-tf-idf-algorithm\n",
    "#tf = count(word, document) / len(document) – term frequency\n",
    "#idf = log( len(collection) / count(document_containing_term, collection) – inverse document frequency )\n",
    "#tf-idf = tf * idf – term frequency – inverse document frequency\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem.isri import ISRIStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "\n",
    "def tokenize_removeStop_stem(text):\n",
    "    stop = stopwords.words(\"arabic\")+list(punctuation)\n",
    "    stemmed_words=[]\n",
    "    stemmed_sent=[]\n",
    "    words = word_tokenize(text)\n",
    "    st = ISRIStemmer()\n",
    "    for word in words:\n",
    "        stemmed_words.append(st.stem(word))\n",
    "        stemmed_sent = \" \".join(stemmed_words)\n",
    "    filtered = []\n",
    "    for word in  stemmed_words:\n",
    "        if word not in stop:\n",
    "            filtered.append(word)\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tf_idf():\n",
    "    #text = [\"أحمد قال هذا من قبل\", \"لا يوجد بشر يرفعون أحمد\", \"هذه كانت خدعة مثلما قال احمد من قبل \", \"لا يوجد بشر يرفعون أحمد\", \"هذه كانت خدعة مثلما قال احمد من قبل \"]\n",
    "    # word tokenize and stem\n",
    "    text=[]\n",
    "    flat_text=[]\n",
    "    file1=open('pathall/D1.txt', encoding='utf-8')\n",
    "    d1=file1.read().replace('\\n', ' ')\n",
    "    file2=open('pathall/D2.txt', encoding='utf-8')\n",
    "    d2=file2.read().replace('\\n', ' ')\n",
    "    file3=open('pathall/D3.txt', encoding='utf-8')\n",
    "    d3=file3.read().replace('\\n', ' ')\n",
    "    file4=open('pathall/D4.txt', encoding='utf-8')\n",
    "    d4=file4.read().replace('\\n', ' ')\n",
    "    text.append(d1)\n",
    "    text.append(d2)\n",
    "    text.append(d3)\n",
    "    text.append(d4)\n",
    "    for item in text:\n",
    "         flat_text.append(item)\n",
    "    print(flat_text)\n",
    "    flat_text = [\" \".join(tokenize_removeStop_stem(txt)) for txt in  flat_text]\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    matrix = vectorizer.fit_transform( flat_text).todense()\n",
    "    # transform the matrix to a pandas df\n",
    "    matrix_pd = pd.DataFrame(matrix, columns=vectorizer.get_feature_names())\n",
    "    # sum over each document (axis=0)\n",
    "    top_words = matrix_pd.sum(axis=0).sort_values(ascending=False)\n",
    "    return(matrix_pd)\n",
    "  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\ufeffيحتوى هذا الجزء على تعليمات', '\\ufeffتحتوى هذه الشاشة على اجزاء مهمة', '\\ufeffما لى حيلة الا رجائى بعفوك ان عفوت', '\\ufeffيحتوى هذا الجزء على تعليمات']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>الا</th>\n",
       "      <th>ان</th>\n",
       "      <th>تحتوى</th>\n",
       "      <th>جزء</th>\n",
       "      <th>حيل</th>\n",
       "      <th>رجى</th>\n",
       "      <th>ششة</th>\n",
       "      <th>عفت</th>\n",
       "      <th>عفو</th>\n",
       "      <th>علم</th>\n",
       "      <th>لى</th>\n",
       "      <th>ما</th>\n",
       "      <th>همة</th>\n",
       "      <th>يحتوى</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.496816</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.613667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.613667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.541736</td>\n",
       "      <td>0.345783</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.541736</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.541736</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.353553</td>\n",
       "      <td>0.353553</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.353553</td>\n",
       "      <td>0.353553</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.353553</td>\n",
       "      <td>0.353553</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.353553</td>\n",
       "      <td>0.353553</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.496816</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.613667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.613667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        الا        ان     تحتوى       جزء       حيل       رجى       ششة  \\\n",
       "0  0.000000  0.000000  0.000000  0.496816  0.000000  0.000000  0.000000   \n",
       "1  0.000000  0.000000  0.541736  0.345783  0.000000  0.000000  0.541736   \n",
       "2  0.353553  0.353553  0.000000  0.000000  0.353553  0.353553  0.000000   \n",
       "3  0.000000  0.000000  0.000000  0.496816  0.000000  0.000000  0.000000   \n",
       "\n",
       "        عفت       عفو       علم        لى        ما       همة     يحتوى  \n",
       "0  0.000000  0.000000  0.613667  0.000000  0.000000  0.000000  0.613667  \n",
       "1  0.000000  0.000000  0.000000  0.000000  0.000000  0.541736  0.000000  \n",
       "2  0.353553  0.353553  0.000000  0.353553  0.353553  0.000000  0.000000  \n",
       "3  0.000000  0.000000  0.613667  0.000000  0.000000  0.000000  0.613667  "
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
